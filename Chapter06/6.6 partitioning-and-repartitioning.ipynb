{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6020138a-0dc8-4b7e-bad8-9cf2ef7133aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession \n",
    "from pyspark.sql.functions import rand, when"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2b24dc6e-9c01-4a8b-9b6e-0d71b953bd82",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/21 13:36:05 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark = (SparkSession.builder\n",
    "         .appName(\"partitioning-and-repartitioning\")\n",
    "         .master(\"spark://spark-master:7077\")\n",
    "         .config(\"spark.executor.memory\", \"512m\")\n",
    "         .getOrCreate())\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "20292d5d-76f1-4974-8c2a-a3c7f891762a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------+------+------------+\n",
      "| id|salary|gender|country_code|\n",
      "+---+------+------+------------+\n",
      "|  0|  2700|     M|          BR|\n",
      "|  1|   500|     M|          IN|\n",
      "|  2|  4200|     M|          BR|\n",
      "|  3|  6700|     M|          US|\n",
      "|  4|   400|     M|          BR|\n",
      "+---+------+------+------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Create some sample data frames\n",
    "# A large data frame with 1 million rows\n",
    "large_df = (spark.range(0, 1000000)\n",
    "            .withColumn(\"salary\", 100*(rand() * 100).cast(\"int\"))\n",
    "            .withColumn(\"gender\", when((rand() * 2).cast(\"int\") == 0, \"M\").otherwise(\"F\"))\n",
    "            .withColumn(\"country_code\", \n",
    "                        when((rand() * 4).cast(\"int\") == 0, \"US\")\n",
    "                        .when((rand() * 4).cast(\"int\") == 1, \"CN\")\n",
    "                        .when((rand() * 4).cast(\"int\") == 2, \"IN\")\n",
    "                        .when((rand() * 4).cast(\"int\") == 3, \"BR\")))\n",
    "large_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "67debace-58d6-471c-946e-beb7bf67d58c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition sizes: [500000, 500000]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_partitions = large_df.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "partition_sizes = large_df.rdd.glom().map(len).collect()\n",
    "print(f\"Partition sizes: {partition_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c884999-9a74-426e-8ab0-4f5f1f9f4793",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_hash = large_df.repartition(10, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b97ff599-9f82-43c1-a63e-7d87cc045970",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:>                                                          (0 + 2) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition sizes: [99990, 99781, 99533, 99938, 100111, 100200, 100448, 100094, 100048, 99857]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_partitions = df_hash.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "partition_sizes = df_hash.rdd.glom().map(len).collect()\n",
    "print(f\"Partition sizes: {partition_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "66dbfaf6-1fc3-43df-ae29-837a3889a802",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_range = large_df.repartitionByRange(10, \"id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0203f7e3-09e2-4a71-b851-7da06d6bb5ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 8:==============================================>           (8 + 2) / 10]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition sizes: [93970, 98146, 108664, 99680, 99522, 106143, 92137, 97096, 112388, 92254]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_partitions = df_range.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "partition_sizes = df_range.rdd.glom().map(len).collect()\n",
    "print(f\"Partition sizes: {partition_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7a1791f7-b716-4bfc-9628-e14676e22ec5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_coalesce = df_range.coalesce(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30b217b4-da5d-4309-9fb1-ed682d77e762",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 12:===========================================>              (3 + 1) / 4]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partition sizes: [205487, 298575, 299793, 196145]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "num_partitions = df_coalesce.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions}\")\n",
    "\n",
    "partition_sizes = df_coalesce.rdd.glom().map(len).collect()\n",
    "print(f\"Partition sizes: {partition_sizes}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21824af2-406b-46a5-bc9c-1890a519be22",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 13:>                                                         (0 + 2) / 2]\r"
     ]
    }
   ],
   "source": [
    "(large_df.write \n",
    "    .format(\"parquet\")\n",
    "    .partitionBy(\"id\")\n",
    "    .mode(\"overwrite\")\n",
    "    .save(\"../data/tmp/partitioned_output\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "038d9bad-49cb-48f6-ba76-2d3b9a2c890e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+---+\n",
      "|     value| id|\n",
      "+----------+---+\n",
      "|0.15517375|505|\n",
      "|0.95623612|505|\n",
      "|0.09070664|505|\n",
      "|0.85489201|505|\n",
      "|0.09197253|505|\n",
      "+----------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_read = (spark.read\n",
    "           .format(\"parquet\")\n",
    "           .load(\"../data/tmp/partitioned_output\"))\n",
    "\n",
    "df_read.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "39884056-8d4d-4616-bd58-91948a3ad93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7d67a5-1fb1-4203-853e-9622761eb96c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
